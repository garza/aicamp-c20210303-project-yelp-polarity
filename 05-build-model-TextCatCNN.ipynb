{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "trained-paint",
   "metadata": {},
   "source": [
    "# 05-build-model-TextCatCNN\n",
    "\n",
    "## Main objectives:\n",
    "\n",
    "- Use spaCy's neural netowkr model where token vectors are calculated using a CNN\n",
    "    - [https://spacy.io/api/architectures#TextCatCNN](https://spacy.io/api/architectures#TextCatCNN)\n",
    "- Use spaCy generated docs to train this model\n",
    "- Run basic validation and evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "current-light",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# load an english language model in spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "motivated-conjunction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev.spacy  train.spacy\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerous-criminal",
   "metadata": {},
   "source": [
    "# Validate configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "employed-means",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "============================= Config validation =============================\u001b[0m\n",
      "\u001b[1m\n",
      "===================== Config validation for [initialize] =====================\u001b[0m\n",
      "\u001b[1m\n",
      "====================== Config validation for [training] ======================\u001b[0m\n",
      "\u001b[38;5;2m‚úî Config is valid\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# validate configuration\n",
    "!python -m spacy debug config ./config/config-TextCatCNN.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "weighted-neutral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "============================ Data file validation ============================\u001b[0m\n",
      "\u001b[38;5;2m‚úî Corpus is loadable\u001b[0m\n",
      "\u001b[38;5;2m‚úî Pipeline can be initialized with data\u001b[0m\n",
      "\u001b[1m\n",
      "=============================== Training stats ===============================\u001b[0m\n",
      "Language: en\n",
      "Training pipeline: textcat\n",
      "5000 training docs\n",
      "5000 evaluation docs\n",
      "\u001b[38;5;2m‚úî No overlap between training and evaluation data\u001b[0m\n",
      "\u001b[1m\n",
      "============================== Vocab & Vectors ==============================\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ 282161 total word(s) in the data (18163 unique)\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ No word vectors present in the package\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Summary ==================================\u001b[0m\n",
      "\u001b[38;5;2m‚úî 3 checks passed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy debug data ./config/config-TextCatCNN.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-netscape",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "quiet-wealth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4m‚Ñπ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2021-03-22 22:42:07,593] [INFO] Set up nlp object from config\n",
      "[2021-03-22 22:42:08,014] [INFO] Pipeline: ['textcat']\n",
      "[2021-03-22 22:42:08,020] [INFO] Created vocabulary\n",
      "[2021-03-22 22:42:08,020] [INFO] Finished initializing nlp object\n",
      "[2021-03-22 22:42:18,341] [INFO] Initialized pipeline components: ['textcat']\n",
      "\u001b[38;5;2m‚úî Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ Pipeline: ['textcat']\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ Initial learn rate: 0.001\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.23\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33meager-hill-18\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/garza/yelp-polarity\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/garza/yelp-polarity/runs/1qd1puec\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/garza/notebooks/aicamp/yelp-project/wandb/run-20210322_224223-1qd1puec\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "E    #       LOSS TEXTCAT  CATS_SCORE  SCORE \n",
      "---  ------  ------------  ----------  ------\n",
      "  0       0          0.50       57.70    0.58\n",
      "  0      10          3.78       58.51    0.59\n",
      "  0      20          2.57       58.33    0.58\n",
      "  0      30          3.93       62.58    0.63\n",
      "  0      40          2.91       67.22    0.67\n",
      "  0      50          3.20       65.75    0.66\n",
      "  0      60          4.40       67.50    0.67\n",
      "  0      70          3.08       62.00    0.62\n",
      "  0      80          3.24       66.99    0.67\n",
      "  0      90          3.64       74.30    0.74\n",
      "  0     100          2.29       79.10    0.79\n",
      "  0     110          4.80       79.17    0.79\n",
      "  0     120          3.69       79.20    0.79\n",
      "  0     130          4.39       80.25    0.80\n",
      "  0     140          1.36       80.15    0.80\n",
      "  0     150          4.87       81.81    0.82\n",
      "  0     160          0.89       81.68    0.82\n",
      "  0     170          1.91       83.30    0.83\n",
      "  0     180          2.35       83.10    0.83\n",
      "  0     190          1.17       82.87    0.83\n",
      "  0     200          2.53       84.34    0.84\n",
      "  0     210          5.59       85.53    0.86\n",
      "  0     220          2.79       85.77    0.86\n",
      "  0     230          2.51       86.55    0.87\n",
      "  0     240          1.50       86.99    0.87\n",
      "  0     250          3.14       86.93    0.87\n",
      "  0     260          4.25       86.62    0.87\n",
      "  0     270          2.14       87.27    0.87\n",
      "  0     280          1.32       87.30    0.87\n",
      "  0     290          1.18       87.60    0.88\n",
      "  0     300          0.76       87.60    0.88\n",
      "  0     310          1.89       87.63    0.88\n",
      "  0     320          0.64       87.32    0.87\n",
      "  0     330          1.97       87.68    0.88\n",
      "  0     340          2.00       88.19    0.88\n",
      "  0     350          2.56       88.48    0.88\n",
      "  0     360          1.61       88.53    0.89\n",
      "  0     370          1.11       88.51    0.89\n",
      "  0     380          1.61       88.48    0.88\n",
      "  0     390          2.40       88.58    0.89\n",
      "  0     400          0.85       88.95    0.89\n",
      "  0     410          0.75       89.06    0.89\n",
      "  0     420          1.12       89.05    0.89\n",
      "  0     430          1.58       89.11    0.89\n",
      "  0     440          0.32       89.15    0.89\n",
      "  0     450          0.88       89.04    0.89\n",
      "  0     460          0.48       88.84    0.89\n",
      "  0     470          0.41       88.78    0.89\n",
      "  0     480          0.18       88.71    0.89\n",
      "  0     490          3.16       88.84    0.89\n",
      "  0     500          1.91       88.76    0.89\n",
      "  0     510          0.85       89.05    0.89\n",
      "  0     520          3.56       89.25    0.89\n",
      "  0     530          1.43       89.33    0.89\n",
      "  0     540          0.27       89.38    0.89\n",
      "  0     550          1.10       89.30    0.89\n",
      "  0     560          1.26       89.34    0.89\n",
      "  0     570          0.21       89.42    0.89\n",
      "  0     580          1.34       89.49    0.89\n",
      "  0     590          1.52       89.26    0.89\n",
      "  0     600          2.05       89.36    0.89\n",
      "  0     610          0.78       89.22    0.89\n",
      "  0     620          3.11       89.38    0.89\n",
      "  0     630          2.18       89.48    0.89\n",
      "  0     640          0.21       89.62    0.90\n",
      "  0     650          2.18       89.62    0.90\n",
      "  0     660          0.27       89.47    0.89\n",
      "  0     670          0.56       89.64    0.90\n",
      "  0     680          0.84       89.69    0.90\n",
      "  0     690          2.09       89.82    0.90\n",
      "  0     700          0.48       89.88    0.90\n",
      "  0     710          0.95       90.17    0.90\n",
      "  0     720          0.87       90.45    0.90\n",
      "  0     730          0.16       90.60    0.91\n",
      "  0     740          0.28       90.67    0.91\n",
      "  0     750          0.76       90.81    0.91\n",
      "  0     760          0.19       90.80    0.91\n",
      "  0     770          0.72       90.69    0.91\n",
      "  0     780          0.14       90.61    0.91\n",
      "  0     790          1.28       90.54    0.91\n",
      "  0     800          0.31       90.45    0.90\n",
      "  0     810          0.51       90.40    0.90\n",
      "  0     820          1.92       90.33    0.90\n",
      "  0     830          0.25       90.30    0.90\n",
      "  0     840          0.92       90.28    0.90\n",
      "  0     850          0.44       90.45    0.90\n",
      "  0     860          0.10       90.66    0.91\n",
      "  0     870          0.43       90.62    0.91\n",
      "  0     880          0.30       90.71    0.91\n",
      "  0     890          0.09       90.78    0.91\n",
      "  0     900          1.47       90.98    0.91\n",
      "  0     910          0.84       91.10    0.91\n",
      "  0     920          0.29       91.12    0.91\n",
      "  0     930          0.29       91.09    0.91\n",
      "  0     940          0.32       91.37    0.91\n",
      "  0     950          0.43       91.43    0.91\n",
      "  0     960          0.44       91.48    0.91\n",
      "  0     970          0.39       91.41    0.91\n",
      "  0     980          0.28       91.37    0.91\n",
      "  0     990          0.45       91.39    0.91\n",
      "  0    1000          0.58       91.42    0.91\n",
      "  0    1010          0.69       91.51    0.92\n",
      "  0    1020          1.36       91.53    0.92\n",
      "  0    1030          0.46       91.53    0.92\n",
      "  0    1040          0.29       91.60    0.92\n",
      "  0    1050          0.25       91.64    0.92\n",
      "  0    1060          0.25       91.59    0.92\n",
      "  0    1070          0.20       91.54    0.92\n",
      "  0    1080          0.10       91.55    0.92\n",
      "  0    1090          0.11       91.59    0.92\n",
      "  0    1100          0.43       91.75    0.92\n",
      "  0    1110          0.05       91.78    0.92\n",
      "  0    1120          0.12       91.65    0.92\n",
      "  0    1130          0.29       91.65    0.92\n",
      "  0    1140          0.25       91.86    0.92\n",
      "  0    1150          0.32       91.89    0.92\n",
      "  0    1160          0.13       91.94    0.92\n",
      "  0    1170          0.19       92.18    0.92\n",
      "  0    1180          0.08       92.40    0.92\n",
      "  0    1190          0.51       92.45    0.92\n",
      "  0    1200          0.27       92.58    0.93\n",
      "  0    1210          0.10       92.69    0.93\n",
      "  0    1220          1.13       92.62    0.93\n",
      "  0    1230          0.10       92.75    0.93\n",
      "  0    1240          0.10       92.76    0.93\n",
      "  0    1250          0.22       92.75    0.93\n",
      "  0    1260          0.04       92.74    0.93\n",
      "  0    1270          0.11       92.73    0.93\n",
      "  0    1280          0.16       92.76    0.93\n",
      "  0    1290          0.07       92.76    0.93\n",
      "  0    1300          0.23       92.74    0.93\n",
      "  0    1310          0.21       92.81    0.93\n",
      "  0    1320          0.09       92.83    0.93\n",
      "  0    1330          0.08       92.86    0.93\n",
      "  0    1340          0.11       92.90    0.93\n",
      "  0    1350          0.24       92.88    0.93\n",
      "  0    1360          1.50       92.82    0.93\n",
      "  0    1370          0.08       92.80    0.93\n",
      "  0    1380          0.09       92.87    0.93\n",
      "  0    1390          0.05       92.93    0.93\n",
      "  0    1400          0.09       92.92    0.93\n",
      "  0    1410          0.06       92.87    0.93\n",
      "  0    1420          0.20       92.87    0.93\n",
      "  0    1430          0.10       92.86    0.93\n",
      "  0    1440          0.15       92.90    0.93\n",
      "  0    1450          0.04       92.95    0.93\n",
      "  0    1460          0.09       92.96    0.93\n",
      "  0    1470          0.04       92.98    0.93\n",
      "  1    1480          0.02       92.99    0.93\n",
      "  1    1490          0.05       92.95    0.93\n",
      "  1    1500          0.04       92.92    0.93\n",
      "  1    1510          0.01       92.93    0.93\n",
      "  1    1520          0.01       92.93    0.93\n",
      "  1    1530          0.03       92.96    0.93\n",
      "  1    1540          0.03       92.99    0.93\n",
      "  1    1550          0.02       93.04    0.93\n",
      "  1    1560          0.03       93.03    0.93\n",
      "  1    1570          0.03       93.01    0.93\n",
      "  1    1580          0.03       92.97    0.93\n",
      "  1    1590          0.02       92.96    0.93\n",
      "  1    1600          0.03       92.91    0.93\n",
      "  1    1610          0.01       92.87    0.93\n",
      "  1    1620          0.01       92.84    0.93\n",
      "  1    1630          0.10       92.85    0.93\n",
      "  1    1640          0.01       92.80    0.93\n",
      "  1    1650          0.02       92.80    0.93\n",
      "  1    1660          0.02       92.84    0.93\n",
      "  1    1670          0.01       92.84    0.93\n",
      "  1    1680          0.02       92.91    0.93\n",
      "  1    1690          0.01       92.88    0.93\n",
      "  1    1700          0.00       92.85    0.93\n",
      "  1    1710          0.01       92.83    0.93\n",
      "  1    1720          0.01       92.78    0.93\n",
      "  1    1730          0.01       92.79    0.93\n",
      "  1    1740          0.02       92.81    0.93\n",
      "  1    1750          0.02       92.82    0.93\n",
      "  1    1760          0.02       92.85    0.93\n",
      "  1    1770          0.01       92.86    0.93\n",
      "  1    1780          0.03       92.87    0.93\n",
      "  1    1790          0.01       92.86    0.93\n",
      "  1    1800          0.01       92.82    0.93\n",
      "  1    1810          0.00       92.77    0.93\n",
      "  1    1820          0.00       92.71    0.93\n",
      "  1    1830          0.02       92.64    0.93\n",
      "  1    1840          0.00       92.65    0.93\n",
      "  1    1850          0.01       92.69    0.93\n",
      "  1    1860          0.01       92.64    0.93\n",
      "  1    1870          0.01       92.68    0.93\n",
      "  1    1880          0.01       92.68    0.93\n",
      "  1    1890          0.01       92.68    0.93\n",
      "  1    1900          0.01       92.68    0.93\n",
      "  1    1910          0.03       92.70    0.93\n",
      "  1    1920          0.01       92.80    0.93\n",
      "  1    1930          0.01       92.86    0.93\n",
      "  1    1940          0.01       92.84    0.93\n",
      "  1    1950          0.01       92.80    0.93\n",
      "  1    1960          0.02       92.63    0.93\n",
      "  1    1970          0.02       92.61    0.93\n",
      "  1    1980          0.01       92.62    0.93\n",
      "  1    1990          0.01       92.69    0.93\n",
      "  1    2000          0.02       92.72    0.93\n",
      "  1    2010          0.00       92.78    0.93\n",
      "  2    2020          0.01       92.78    0.93\n",
      "  2    2030          0.00       92.74    0.93\n",
      "  2    2040          0.00       92.74    0.93\n",
      "  2    2050          0.00       92.73    0.93\n",
      "  2    2060          0.00       92.70    0.93\n",
      "  2    2070          0.00       92.68    0.93\n",
      "  2    2080          0.00       92.56    0.93\n",
      "  2    2090          0.00       92.37    0.92\n",
      "  2    2100          0.00       92.29    0.92\n",
      "  2    2110          0.00       92.36    0.92\n",
      "  2    2120          0.00       92.41    0.92\n",
      "  2    2130          0.00       92.49    0.92\n",
      "  2    2140          0.00       92.51    0.93\n",
      "  2    2150          0.00       92.55    0.93\n",
      "  2    2160          0.00       92.58    0.93\n",
      "  2    2170          0.00       92.55    0.93\n",
      "  2    2180          0.00       92.51    0.93\n",
      "  2    2190          0.00       92.54    0.93\n",
      "  2    2200          0.00       92.54    0.93\n",
      "  2    2210          0.00       92.54    0.93\n",
      "  2    2220          0.00       92.51    0.93\n",
      "  2    2230          0.00       92.57    0.93\n",
      "  2    2240          0.00       92.62    0.93\n",
      "  2    2250          0.00       92.66    0.93\n",
      "  2    2260          0.00       92.63    0.93\n",
      "  2    2270          0.00       92.64    0.93\n",
      "  2    2280          0.00       92.62    0.93\n",
      "  2    2290          0.00       92.58    0.93\n",
      "  2    2300          0.00       92.52    0.93\n",
      "  2    2310          0.00       92.54    0.93\n",
      "  2    2320          0.00       92.55    0.93\n",
      "  2    2330          0.00       92.55    0.93\n",
      "  2    2340          0.00       92.54    0.93\n",
      "  2    2350          0.00       92.54    0.93\n",
      "  2    2360          0.00       92.48    0.92\n",
      "  3    2370          0.00       92.39    0.92\n",
      "  3    2380          0.00       92.38    0.92\n",
      "  3    2390          0.00       92.41    0.92\n",
      "  3    2400          0.00       92.39    0.92\n",
      "  3    2410          0.00       92.41    0.92\n",
      "  3    2420          0.00       92.42    0.92\n",
      "  3    2430          0.00       92.27    0.92\n",
      "  3    2440          0.00       92.16    0.92\n",
      "  3    2450          0.00       92.13    0.92\n",
      "  3    2460          0.00       92.16    0.92\n",
      "  3    2470          0.00       92.19    0.92\n",
      "  3    2480          0.00       92.24    0.92\n",
      "  3    2490          0.00       92.25    0.92\n",
      "  3    2500          0.00       92.27    0.92\n",
      "  3    2510          0.00       92.29    0.92\n",
      "  3    2520          0.00       92.30    0.92\n",
      "  3    2530          0.00       92.32    0.92\n",
      "  3    2540          0.00       92.33    0.92\n",
      "  3    2550          0.00       92.34    0.92\n",
      "  3    2560          0.00       92.31    0.92\n",
      "  3    2570          0.00       92.30    0.92\n",
      "  3    2580          0.00       92.30    0.92\n",
      "  3    2590          0.00       92.29    0.92\n",
      "  3    2600          0.00       92.30    0.92\n",
      "  3    2610          0.00       92.29    0.92\n",
      "  3    2620          0.00       92.28    0.92\n",
      "  3    2630          0.00       92.31    0.92\n",
      "  3    2640          0.00       92.31    0.92\n",
      "  3    2650          0.00       92.25    0.92\n",
      "  4    2660          0.00       92.20    0.92\n",
      "  4    2670          0.00       92.18    0.92\n",
      "  4    2680          0.00       92.18    0.92\n",
      "  4    2690          0.00       92.16    0.92\n",
      "  4    2700          0.00       92.17    0.92\n",
      "  4    2710          0.00       92.17    0.92\n",
      "  4    2720          0.00       92.14    0.92\n",
      "  4    2730          0.00       92.14    0.92\n",
      "  4    2740          0.00       92.14    0.92\n",
      "  4    2750          0.00       92.14    0.92\n",
      "  4    2760          0.00       92.14    0.92\n",
      "  4    2770          0.00       92.14    0.92\n",
      "  4    2780          0.00       92.14    0.92\n",
      "  4    2790          0.00       92.12    0.92\n",
      "  4    2800          0.00       92.10    0.92\n",
      "  4    2810          0.00       92.11    0.92\n",
      "  4    2820          0.00       92.10    0.92\n",
      "  4    2830          0.00       92.08    0.92\n",
      "  4    2840          0.00       92.08    0.92\n",
      "  4    2850          0.00       92.09    0.92\n",
      "  4    2860          0.00       92.10    0.92\n",
      "  4    2870          0.00       92.08    0.92\n",
      "  4    2880          0.00       92.07    0.92\n",
      "  4    2890          0.00       92.05    0.92\n",
      "  4    2900          0.00       92.04    0.92\n",
      "  4    2910          0.00       92.04    0.92\n",
      "  4    2920          0.00       92.04    0.92\n",
      "  4    2930          0.00       92.06    0.92\n",
      "  4    2940          0.00       92.07    0.92\n",
      "  4    2950          0.00       92.21    0.92\n",
      "  5    2960          0.00       92.27    0.92\n",
      "  5    2970          0.00       92.27    0.92\n",
      "  5    2980          0.00       92.26    0.92\n",
      "  5    2990          0.00       92.26    0.92\n",
      "  5    3000          0.00       92.25    0.92\n",
      "  5    3010          0.00       92.26    0.92\n",
      "  5    3020          0.00       92.25    0.92\n",
      "  5    3030          0.00       92.24    0.92\n",
      "  5    3040          0.00       92.24    0.92\n",
      "  5    3050          0.00       92.23    0.92\n",
      "  5    3060          0.00       92.17    0.92\n",
      "  5    3070          0.00       92.08    0.92\n",
      "  5    3080          0.00       92.11    0.92\n",
      "  5    3090          0.00       92.10    0.92\n",
      "  5    3100          0.00       92.09    0.92\n",
      "  5    3110          0.00       92.08    0.92\n",
      "  5    3120          0.00       92.11    0.92\n",
      "  5    3130          0.00       92.25    0.92\n",
      "  5    3140          0.00       92.27    0.92\n",
      "  5    3150          0.00       92.24    0.92\n",
      "  5    3160          0.00       92.22    0.92\n",
      "  5    3170          0.00       92.19    0.92\n",
      "  5    3180          0.00       92.10    0.92\n",
      "  5    3190          0.00       92.04    0.92\n",
      "  5    3200          0.00       91.98    0.92\n",
      "  5    3210          0.00       91.98    0.92\n",
      "  5    3220          0.00       91.97    0.92\n",
      "  5    3230          0.00       92.00    0.92\n",
      "  5    3240          0.00       91.99    0.92\n",
      "  5    3250          0.00       91.97    0.92\n",
      "  6    3260          0.00       91.97    0.92\n",
      "  6    3270          0.00       91.99    0.92\n",
      "  6    3280          0.00       92.00    0.92\n",
      "  6    3290          0.00       91.98    0.92\n",
      "  6    3300          0.00       91.98    0.92\n",
      "  6    3310          0.00       91.99    0.92\n",
      "  6    3320          0.00       91.99    0.92\n",
      "  6    3330          0.00       91.99    0.92\n",
      "  6    3340          0.00       92.00    0.92\n",
      "  6    3350          0.00       91.97    0.92\n",
      "  6    3360          0.00       91.95    0.92\n",
      "  6    3370          0.00       91.98    0.92\n",
      "  6    3380          0.00       91.98    0.92\n",
      "  6    3390          0.00       92.02    0.92\n",
      "  6    3400          0.00       92.05    0.92\n",
      "  6    3410          0.00       92.06    0.92\n",
      "  6    3420          0.00       92.07    0.92\n",
      "  6    3430          0.00       92.00    0.92\n",
      "  6    3440          0.00       92.01    0.92\n",
      "  6    3450          0.00       91.96    0.92\n",
      "  6    3460          0.00       91.91    0.92\n",
      "  6    3470          0.00       91.88    0.92\n",
      "  6    3480          0.00       91.96    0.92\n",
      "  6    3490          0.00       91.89    0.92\n",
      "  6    3500          0.00       91.90    0.92\n",
      "  6    3510          0.00       91.87    0.92\n",
      "  6    3520          0.00       91.85    0.92\n",
      "  6    3530          0.00       91.86    0.92\n",
      "  6    3540          0.00       91.94    0.92\n",
      "  7    3550          0.00       92.05    0.92\n",
      "  7    3560          0.00       92.02    0.92\n",
      "  7    3570          0.00       92.02    0.92\n",
      "  7    3580          0.00       92.03    0.92\n",
      "  7    3590          0.00       92.09    0.92\n",
      "  7    3600          0.00       92.07    0.92\n",
      "  7    3610          0.00       92.06    0.92\n",
      "  7    3620          0.00       92.03    0.92\n",
      "  7    3630          0.00       92.01    0.92\n",
      "  7    3640          0.00       92.03    0.92\n",
      "  7    3650          0.00       92.05    0.92\n",
      "  7    3660          0.00       92.02    0.92\n",
      "  7    3670          0.00       91.92    0.92\n",
      "  7    3680          0.00       91.84    0.92\n",
      "  7    3690          0.00       91.83    0.92\n",
      "  7    3700          0.00       91.88    0.92\n",
      "  7    3710          0.00       91.84    0.92\n",
      "  7    3720          0.00       91.85    0.92\n",
      "  7    3730          0.00       91.84    0.92\n",
      "  7    3740          0.00       91.86    0.92\n",
      "  7    3750          0.00       91.83    0.92\n",
      "  7    3760          0.00       91.74    0.92\n",
      "  7    3770          0.00       91.62    0.92\n",
      "  7    3780          0.00       91.50    0.92\n",
      "  7    3790          0.00       91.65    0.92\n",
      "  7    3800          0.00       91.60    0.92\n",
      "  7    3810          0.00       91.51    0.92\n",
      "  7    3820          0.00       91.49    0.91\n",
      "  7    3830          0.00       91.46    0.91\n",
      "  7    3840          0.00       91.52    0.92\n",
      "  8    3850          0.00       91.49    0.91\n",
      "  8    3860          0.00       91.52    0.92\n",
      "  8    3870          0.00       91.63    0.92\n",
      "  8    3880          0.00       91.63    0.92\n",
      "  8    3890          0.00       91.61    0.92\n",
      "  8    3900          0.00       91.67    0.92\n",
      "  8    3910          0.00       91.70    0.92\n",
      "  8    3920          0.00       91.71    0.92\n",
      "  8    3930          0.00       91.70    0.92\n",
      "  8    3940          0.00       91.70    0.92\n",
      "  8    3950          0.00       91.69    0.92\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 1989\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/garza/notebooks/aicamp/yelp-project/wandb/run-20210322_224223-1qd1puec/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/garza/notebooks/aicamp/yelp-project/wandb/run-20210322_224223-1qd1puec/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               score 0.91686\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _runtime 13191\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          _timestamp 1616484134\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               _step 1187\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        loss_textcat 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           token_acc 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             token_p 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             token_r 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             token_f 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          cats_score 0.91686\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     cats_score_desc macro AUC\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        cats_micro_p 0.845\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        cats_micro_r 0.845\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        cats_micro_f 0.845\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        cats_macro_p 0.84566\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        cats_macro_r 0.845\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        cats_macro_f 0.84493\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      cats_macro_auc 0.91686\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               speed 15638.96291\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            score ‚ñÅ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         _runtime ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       _timestamp ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     loss_textcat ‚ñá‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        token_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          token_p ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          token_r ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          token_f ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       cats_score ‚ñÅ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     cats_micro_p ‚ñÅ‚ñÇ‚ñÉ‚ñá‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     cats_micro_r ‚ñÅ‚ñÇ‚ñÉ‚ñá‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     cats_micro_f ‚ñÅ‚ñÇ‚ñÉ‚ñá‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     cats_macro_p ‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     cats_macro_r ‚ñÅ‚ñÇ‚ñÉ‚ñá‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     cats_macro_f ‚ñÅ‚ñÇ‚ñÑ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   cats_macro_auc ‚ñÅ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            speed ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33meager-hill-18\u001b[0m: \u001b[34mhttps://wandb.ai/garza/yelp-polarity/runs/1qd1puec\u001b[0m\n",
      "\u001b[38;5;2m‚úî Saved pipeline to output directory\u001b[0m\n",
      "models/textCatCNN/model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train ./config/config-TextCatCNN.cfg --output ./models/textCatCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-subcommittee",
   "metadata": {},
   "source": [
    "# Evaluate our best model output and save metrics to disk\n",
    "\n",
    "For hyperparameter tuning, experimented with different tok2vec attributes and row sizes settling on:\n",
    "\n",
    "- \"NORM\", \"PREFIX\", \"SUFFIX\", \"SHAPE\"\n",
    "- [10000, 5000, 5000, 5000]\n",
    "\n",
    "Also adjusted the width size from 64 to 96, which only resulted in a nominal increase in performance.\n",
    "\n",
    "Training was tested on training datasets of 100, 500, 1000 and finally, 5000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "tutorial-audience",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4m‚Ñπ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Results ==================================\u001b[0m\n",
      "\n",
      "TOK                   100.00\n",
      "TEXTCAT (macro AUC)   93.04 \n",
      "SPEED                 17016 \n",
      "\n",
      "\u001b[1m\n",
      "=========================== Textcat F (per label) ===========================\u001b[0m\n",
      "\n",
      "               P       R       F\n",
      "positive   86.45   84.44   85.43\n",
      "negative   84.79   86.76   85.77\n",
      "\n",
      "\u001b[1m\n",
      "======================== Textcat ROC AUC (per label) ========================\u001b[0m\n",
      "\n",
      "           ROC AUC\n",
      "positive      0.93\n",
      "negative      0.93\n",
      "\n",
      "\u001b[38;5;2m‚úî Saved results to evaluate/model-textCatCNN-metrics.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy evaluate ./models/textCatCNN/model-best ./data/dev.spacy --output ./evaluate/model-textCatCNN-metrics.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "pending-arrest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "============================ Data file validation ============================\u001b[0m\n",
      "\u001b[38;5;2m‚úî Corpus is loadable\u001b[0m\n",
      "\u001b[38;5;2m‚úî Pipeline can be initialized with data\u001b[0m\n",
      "\u001b[1m\n",
      "=============================== Training stats ===============================\u001b[0m\n",
      "Language: en\n",
      "Training pipeline: textcat\n",
      "5000 training docs\n",
      "5000 evaluation docs\n",
      "\u001b[38;5;2m‚úî No overlap between training and evaluation data\u001b[0m\n",
      "\u001b[1m\n",
      "============================== Vocab & Vectors ==============================\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ 282161 total word(s) in the data (18163 unique)\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ No word vectors present in the package\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Summary ==================================\u001b[0m\n",
      "\u001b[38;5;2m‚úî 3 checks passed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy debug data ./config/config-TextCatCNN.cfg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-entrance",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
