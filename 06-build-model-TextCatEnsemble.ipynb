{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "placed-princess",
   "metadata": {},
   "source": [
    "# 06-build-model-TextCatEnsemble\n",
    "\n",
    "## Main objectives:\n",
    "\n",
    "- Use spaCy's TextCatEnsemble to train a stacked ensemble of a linear bag-of-words model and a neural network model.\n",
    "    - [https://spacy.io/api/architectures#TextCatEnsemble](https://spacy.io/api/architectures#TextCatEnsemble)\n",
    "- Use spaCy generated docs to train this model\n",
    "- Run basic validation and evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "assisted-migration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# load an english language model in spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "indie-burlington",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev.spacy  train.spacy\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-sunglasses",
   "metadata": {},
   "source": [
    "# Validate configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "registered-wheel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "============================= Config validation =============================\u001b[0m\n",
      "\u001b[1m\n",
      "===================== Config validation for [initialize] =====================\u001b[0m\n",
      "\u001b[1m\n",
      "====================== Config validation for [training] ======================\u001b[0m\n",
      "\u001b[38;5;2m‚úî Config is valid\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# validate configuration\n",
    "!python -m spacy debug config ./config/config-TextCatEnsemble.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "celtic-payment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "============================ Data file validation ============================\u001b[0m\n",
      "\u001b[38;5;2m‚úî Corpus is loadable\u001b[0m\n",
      "\u001b[38;5;2m‚úî Pipeline can be initialized with data\u001b[0m\n",
      "\u001b[1m\n",
      "=============================== Training stats ===============================\u001b[0m\n",
      "Language: en\n",
      "Training pipeline: tok2vec, textcat\n",
      "5000 training docs\n",
      "5000 evaluation docs\n",
      "\u001b[38;5;2m‚úî No overlap between training and evaluation data\u001b[0m\n",
      "\u001b[1m\n",
      "============================== Vocab & Vectors ==============================\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ 282161 total word(s) in the data (18163 unique)\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ No word vectors present in the package\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Summary ==================================\u001b[0m\n",
      "\u001b[38;5;2m‚úî 3 checks passed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy debug data ./config/config-TextCatEnsemble.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-produce",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "correct-recognition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4m‚Ñπ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2021-03-22 22:42:33,917] [INFO] Set up nlp object from config\n",
      "[2021-03-22 22:42:34,439] [INFO] Pipeline: ['tok2vec', 'textcat']\n",
      "[2021-03-22 22:42:34,459] [INFO] Created vocabulary\n",
      "[2021-03-22 22:42:34,459] [INFO] Finished initializing nlp object\n",
      "[2021-03-22 22:42:49,172] [INFO] Initialized pipeline components: ['tok2vec', 'textcat']\n",
      "\u001b[38;5;2m‚úî Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ Pipeline: ['tok2vec', 'textcat']\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ Initial learn rate: 0.001\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.23\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtrue-bird-19\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/garza/yelp-polarity\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/garza/yelp-polarity/runs/31v8ob3l\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/garza/notebooks/aicamp/yelp-project/wandb/run-20210322_224254-31v8ob3l\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "E    #       LOSS TOK2VEC  LOSS TEXTCAT  CATS_SCORE  SCORE \n",
      "---  ------  ------------  ------------  ----------  ------\n",
      "  0       0          0.00          0.50       50.62    0.51\n",
      "  0      10          0.00          3.71       51.88    0.52\n",
      "  0      20          0.00          2.59       57.91    0.58\n",
      "  0      30          0.00          3.96       60.87    0.61\n",
      "  0      40          0.00          2.98       64.33    0.64\n",
      "  0      50          0.00          3.06       60.77    0.61\n",
      "  0      60          0.00          4.20       60.70    0.61\n",
      "  0      70          0.00          3.32       65.62    0.66\n",
      "  0      80          0.00          3.32       67.08    0.67\n",
      "  0      90          0.00          3.78       72.20    0.72\n",
      "  0     100          0.00          2.47       72.61    0.73\n",
      "  0     110          0.00          4.67       72.05    0.72\n",
      "  0     120          0.00          3.71       74.16    0.74\n",
      "  0     130          0.00          4.55       73.78    0.74\n",
      "  0     140          0.00          2.06       71.06    0.71\n",
      "  0     150          0.00          4.52       72.09    0.72\n",
      "  0     160          0.00          0.81       73.94    0.74\n",
      "  0     170          0.00          1.93       76.05    0.76\n",
      "  0     180          0.00          2.31       77.46    0.77\n",
      "  0     190          0.00          1.81       77.38    0.77\n",
      "  0     200          0.00          2.70       77.08    0.77\n",
      "  0     210          0.00          4.06       77.03    0.77\n",
      "  0     220          0.00          3.26       78.52    0.79\n",
      "  0     230          0.00          2.22       77.41    0.77\n",
      "  0     240          0.00          1.90       78.86    0.79\n",
      "  0     250          0.00          4.27       78.38    0.78\n",
      "  0     260          0.00          2.94       76.91    0.77\n",
      "  0     270          0.00          2.41       74.33    0.74\n",
      "  0     280          0.00          1.40       70.80    0.71\n",
      "  0     290          0.00          2.38       73.16    0.73\n",
      "  0     300          0.00          2.50       76.46    0.76\n",
      "  0     310          0.00          1.36       78.21    0.78\n",
      "  0     320          0.00          2.40       79.68    0.80\n",
      "  0     330          0.00          2.03       80.49    0.80\n",
      "  0     340          0.00          2.04       79.88    0.80\n",
      "  0     350          0.00          2.36       80.22    0.80\n",
      "  0     360          0.00          1.41       80.67    0.81\n",
      "  0     370          0.00          1.99       80.97    0.81\n",
      "  0     380          0.00          1.79       81.35    0.81\n",
      "  0     390          0.00          3.71       81.77    0.82\n",
      "  0     400          0.00          0.68       81.10    0.81\n",
      "  0     410          0.00          1.95       81.86    0.82\n",
      "  0     420          0.00          1.94       81.62    0.82\n",
      "  0     430          0.00          1.50       80.78    0.81\n",
      "  0     440          0.00          2.11       80.18    0.80\n",
      "  0     450          0.00          1.77       81.24    0.81\n",
      "  0     460          0.00          0.66       81.96    0.82\n",
      "  0     470          0.00          2.99       82.73    0.83\n",
      "  0     480          0.00          2.43       83.19    0.83\n",
      "  0     490          0.00          1.82       83.24    0.83\n",
      "  0     500          0.00          0.86       82.73    0.83\n",
      "  0     510          0.00          2.46       82.99    0.83\n",
      "  0     520          0.00          2.88       82.84    0.83\n",
      "  0     530          0.00          1.17       82.39    0.82\n",
      "  0     540          0.00          0.81       82.36    0.82\n",
      "  0     550          0.00          2.88       81.99    0.82\n",
      "  0     560          0.00          0.77       81.89    0.82\n",
      "  0     570          0.00          0.35       81.55    0.82\n",
      "  0     580          0.00          2.32       82.25    0.82\n",
      "  0     590          0.00          2.51       84.00    0.84\n",
      "  0     600          0.00          2.83       83.70    0.84\n",
      "  0     610          0.00          1.75       82.45    0.82\n",
      "  0     620          0.00          4.47       83.95    0.84\n",
      "  0     630          0.00          1.25       84.37    0.84\n",
      "  0     640          0.00          0.46       84.41    0.84\n",
      "  0     650          0.00          1.02       84.28    0.84\n",
      "  0     660          0.00          0.47       84.26    0.84\n",
      "  0     670          0.00          0.35       84.09    0.84\n",
      "  0     680          0.00          0.84       84.04    0.84\n",
      "  0     690          0.00          0.61       84.31    0.84\n",
      "  0     700          0.00          0.43       84.63    0.85\n",
      "  0     710          0.00          1.79       84.54    0.85\n",
      "  0     720          0.00          3.05       83.78    0.84\n",
      "  0     730          0.00          0.45       84.41    0.84\n",
      "  0     740          0.00          0.60       84.94    0.85\n",
      "  0     750          0.00          0.58       85.55    0.86\n",
      "  0     760          0.00          0.17       85.95    0.86\n",
      "  0     770          0.00          1.01       86.13    0.86\n",
      "  0     780          0.00          0.28       86.12    0.86\n",
      "  0     790          0.00          0.74       86.16    0.86\n",
      "  0     800          0.00          0.20       86.11    0.86\n",
      "  0     810          0.00          1.04       86.16    0.86\n",
      "  0     820          0.00          2.19       86.07    0.86\n",
      "  0     830          0.00          1.06       85.62    0.86\n",
      "  0     840          0.00          2.04       85.29    0.85\n",
      "  0     850          0.00          0.34       85.59    0.86\n",
      "  0     860          0.00          0.11       85.76    0.86\n",
      "  0     870          0.00          0.68       85.83    0.86\n",
      "  0     880          0.00          0.34       85.67    0.86\n",
      "  0     890          0.00          0.19       85.22    0.85\n",
      "  0     900          0.00          1.76       85.14    0.85\n",
      "  0     910          0.00          0.50       85.92    0.86\n",
      "  0     920          0.00          0.45       85.60    0.86\n",
      "  0     930          0.00          0.24       85.32    0.85\n",
      "  0     940          0.00          0.44       84.19    0.84\n",
      "  0     950          0.00          0.52       85.33    0.85\n",
      "  0     960          0.00          0.92       85.29    0.85\n",
      "  0     970          0.00          0.25       85.80    0.86\n",
      "  0     980          0.00          0.40       86.80    0.87\n",
      "  0     990          0.00          0.57       87.65    0.88\n",
      "  0    1000          0.00          0.45       88.04    0.88\n",
      "  0    1010          0.00          0.76       88.21    0.88\n",
      "  0    1020          0.00          1.99       88.23    0.88\n",
      "  0    1030          0.00          0.38       88.85    0.89\n",
      "  0    1040          0.00          0.45       88.97    0.89\n",
      "  0    1050          0.00          0.37       88.83    0.89\n",
      "  0    1060          0.00          0.24       89.23    0.89\n",
      "  0    1070          0.00          0.47       89.17    0.89\n",
      "  0    1080          0.00          0.10       88.97    0.89\n",
      "  0    1090          0.00          0.57       88.81    0.89\n",
      "  0    1100          0.00          0.29       88.36    0.88\n",
      "  0    1110          0.00          0.31       88.11    0.88\n",
      "  0    1120          0.00          0.27       88.72    0.89\n",
      "  0    1130          0.00          0.11       89.19    0.89\n",
      "  0    1140          0.00          0.11       89.80    0.90\n",
      "  0    1150          0.00          0.39       89.97    0.90\n",
      "  0    1160          0.00          0.13       89.94    0.90\n",
      "  0    1170          0.00          0.26       90.12    0.90\n",
      "  0    1180          0.00          0.09       90.23    0.90\n",
      "  0    1190          0.00          1.07       90.23    0.90\n",
      "  0    1200          0.00          0.31       90.26    0.90\n",
      "  0    1210          0.00          0.12       90.37    0.90\n",
      "  0    1220          0.00          0.89       90.34    0.90\n",
      "  0    1230          0.00          0.14       90.30    0.90\n",
      "  0    1240          0.00          0.13       90.36    0.90\n",
      "  0    1250          0.00          0.17       90.35    0.90\n",
      "  0    1260          0.00          0.12       90.36    0.90\n",
      "  0    1270          0.00          0.31       90.51    0.91\n",
      "  0    1280          0.00          0.19       90.73    0.91\n",
      "  0    1290          0.00          0.05       90.68    0.91\n",
      "  0    1300          0.00          0.20       90.53    0.91\n",
      "  0    1310          0.00          0.16       90.56    0.91\n",
      "  0    1320          0.00          0.13       90.50    0.91\n",
      "  0    1330          0.00          0.08       90.74    0.91\n",
      "  0    1340          0.00          0.16       91.08    0.91\n",
      "  0    1350          0.00          0.27       91.13    0.91\n",
      "  0    1360          0.00          1.93       91.42    0.91\n",
      "  0    1370          0.00          0.07       91.63    0.92\n",
      "  0    1380          0.00          0.08       91.89    0.92\n",
      "  0    1390          0.00          0.06       91.77    0.92\n",
      "  0    1400          0.00          0.12       91.89    0.92\n",
      "  0    1410          0.00          0.06       91.86    0.92\n",
      "  0    1420          0.00          1.77       91.58    0.92\n",
      "  0    1430          0.00          0.20       91.55    0.92\n",
      "  0    1440          0.00          0.17       91.53    0.92\n",
      "  0    1450          0.00          0.07       91.79    0.92\n",
      "  0    1460          0.00          0.13       91.83    0.92\n",
      "  0    1470          0.00          0.05       91.76    0.92\n",
      "  1    1480          0.00          0.03       91.86    0.92\n",
      "  1    1490          0.00          0.10       91.82    0.92\n",
      "  1    1500          0.00          0.04       91.74    0.92\n",
      "  1    1510          0.00          0.05       91.97    0.92\n",
      "  1    1520          0.00          0.03       92.20    0.92\n",
      "  1    1530          0.00          0.05       92.38    0.92\n",
      "  1    1540          0.00          0.02       92.45    0.92\n",
      "  1    1550          0.00          0.04       92.35    0.92\n",
      "  1    1560          0.00          0.03       92.44    0.92\n",
      "  1    1570          0.00          0.04       92.57    0.93\n",
      "  1    1580          0.00          0.05       92.62    0.93\n",
      "  1    1590          0.00          0.02       92.68    0.93\n",
      "  1    1600          0.00          0.04       92.73    0.93\n",
      "  1    1610          0.00          0.04       92.74    0.93\n",
      "  1    1620          0.00          0.02       92.66    0.93\n",
      "  1    1630          0.00          0.07       92.59    0.93\n",
      "  1    1640          0.00          0.03       92.52    0.93\n",
      "  1    1650          0.00          0.04       92.58    0.93\n",
      "  1    1660          0.00          0.06       92.25    0.92\n",
      "  1    1670          0.00          0.02       92.15    0.92\n",
      "  1    1680          0.00          0.03       92.35    0.92\n",
      "  1    1690          0.00          0.04       92.13    0.92\n",
      "  1    1700          0.00          0.02       92.06    0.92\n",
      "  1    1710          0.00          0.08       92.00    0.92\n",
      "  1    1720          0.00          0.03       92.08    0.92\n",
      "  1    1730          0.00          0.03       92.06    0.92\n",
      "  1    1740          0.00          0.02       91.96    0.92\n",
      "  1    1750          0.00          0.05       92.27    0.92\n",
      "  1    1760          0.00          0.03       92.33    0.92\n",
      "  1    1770          0.00          0.01       92.32    0.92\n",
      "  1    1780          0.00          0.01       92.34    0.92\n",
      "  1    1790          0.00          0.02       92.38    0.92\n",
      "  1    1800          0.00          0.02       92.59    0.93\n",
      "  1    1810          0.00          0.02       92.54    0.93\n",
      "  1    1820          0.00          0.03       92.68    0.93\n",
      "  1    1830          0.00          0.04       92.65    0.93\n",
      "  1    1840          0.00          0.02       92.85    0.93\n",
      "  1    1850          0.00          0.01       92.67    0.93\n",
      "  1    1860          0.00          0.02       92.73    0.93\n",
      "  1    1870          0.00          0.04       92.77    0.93\n",
      "  1    1880          0.00          0.02       92.85    0.93\n",
      "  1    1890          0.00          0.01       92.76    0.93\n",
      "  1    1900          0.00          0.01       92.80    0.93\n",
      "  1    1910          0.00          0.04       92.77    0.93\n",
      "  1    1920          0.00          0.01       92.65    0.93\n",
      "  1    1930          0.00          0.03       92.57    0.93\n",
      "  1    1940          0.00          0.03       92.56    0.93\n",
      "  1    1950          0.00          0.02       92.35    0.92\n",
      "  1    1960          0.00          0.03       92.46    0.92\n",
      "  1    1970          0.00          0.03       92.27    0.92\n",
      "  1    1980          0.00          0.01       92.37    0.92\n",
      "  1    1990          0.00          0.02       92.38    0.92\n",
      "  1    2000          0.00          0.01       92.09    0.92\n",
      "  1    2010          0.00          0.01       92.35    0.92\n",
      "  2    2020          0.00          0.05       92.45    0.92\n",
      "  2    2030          0.00          0.01       92.35    0.92\n",
      "  2    2040          0.00          0.00       92.30    0.92\n",
      "  2    2050          0.00          0.01       92.34    0.92\n",
      "  2    2060          0.00          0.00       92.26    0.92\n",
      "  2    2070          0.00          0.01       92.06    0.92\n",
      "  2    2080          0.00          0.01       92.06    0.92\n",
      "  2    2090          0.00          0.01       92.00    0.92\n",
      "  2    2100          0.00          0.01       92.00    0.92\n",
      "  2    2110          0.00          0.01       91.87    0.92\n",
      "  2    2120          0.00          0.01       91.82    0.92\n",
      "  2    2130          0.00          0.00       91.84    0.92\n",
      "  2    2140          0.00          0.01       91.79    0.92\n",
      "  2    2150          0.00          0.01       91.93    0.92\n",
      "  2    2160          0.00          0.01       91.90    0.92\n",
      "  2    2170          0.00          0.01       91.93    0.92\n",
      "  2    2180          0.00          0.00       91.91    0.92\n",
      "  2    2190          0.00          0.01       91.91    0.92\n",
      "  2    2200          0.00          0.01       91.87    0.92\n",
      "  2    2210          0.00          0.01       91.91    0.92\n",
      "  2    2220          0.00          0.00       91.87    0.92\n",
      "  2    2230          0.00          0.00       91.82    0.92\n",
      "  2    2240          0.00          0.00       92.04    0.92\n",
      "  2    2250          0.00          0.00       92.08    0.92\n",
      "  2    2260          0.00          0.00       92.13    0.92\n",
      "  2    2270          0.00          0.00       92.18    0.92\n",
      "  2    2280          0.00          0.01       92.19    0.92\n",
      "  2    2290          0.00          0.00       92.13    0.92\n",
      "  2    2300          0.00          0.00       92.16    0.92\n",
      "  2    2310          0.00          0.00       92.18    0.92\n",
      "  2    2320          0.00          0.01       92.07    0.92\n",
      "  2    2330          0.00          0.01       92.04    0.92\n",
      "  2    2340          0.00          0.00       91.93    0.92\n",
      "  2    2350          0.00          0.00       91.93    0.92\n",
      "  2    2360          0.00          0.00       91.92    0.92\n",
      "  3    2370          0.00          0.00       92.02    0.92\n",
      "  3    2380          0.00          0.00       92.13    0.92\n",
      "  3    2390          0.00          0.00       92.22    0.92\n",
      "  3    2400          0.00          0.00       92.16    0.92\n",
      "  3    2410          0.00          0.00       92.09    0.92\n",
      "  3    2420          0.00          0.00       92.02    0.92\n",
      "  3    2430          0.00          0.00       91.95    0.92\n",
      "  3    2440          0.00          0.00       91.89    0.92\n",
      "  3    2450          0.00          0.00       91.63    0.92\n",
      "  3    2460          0.00          0.00       91.83    0.92\n",
      "  3    2470          0.00          0.00       91.76    0.92\n",
      "  3    2480          0.00          0.00       91.72    0.92\n",
      "  3    2490          0.00          0.00       91.37    0.91\n",
      "  3    2500          0.00          0.00       91.49    0.91\n",
      "  3    2510          0.00          0.00       91.51    0.92\n",
      "  3    2520          0.00          0.00       91.65    0.92\n",
      "  3    2530          0.00          0.00       91.61    0.92\n",
      "  3    2540          0.00          0.00       91.70    0.92\n",
      "  3    2550          0.00          0.00       91.74    0.92\n",
      "  3    2560          0.00          0.01       91.60    0.92\n",
      "  3    2570          0.00          0.00       91.42    0.91\n",
      "  3    2580          0.00          0.00       91.49    0.91\n",
      "  3    2590          0.00          0.00       91.64    0.92\n",
      "  3    2600          0.00          0.00       91.72    0.92\n",
      "  3    2610          0.00          0.00       91.80    0.92\n",
      "  3    2620          0.00          0.01       92.05    0.92\n",
      "  3    2630          0.00          0.00       92.14    0.92\n",
      "  3    2640          0.00          0.00       92.19    0.92\n",
      "  3    2650          0.00          0.00       92.08    0.92\n",
      "  4    2660          0.00          0.01       91.98    0.92\n",
      "  4    2670          0.00          0.00       91.90    0.92\n",
      "  4    2680          0.00          0.00       91.69    0.92\n",
      "  4    2690          0.00          0.00       91.68    0.92\n",
      "  4    2700          0.00          0.00       91.63    0.92\n",
      "  4    2710          0.00          0.00       91.56    0.92\n",
      "  4    2720          0.00          0.00       91.61    0.92\n",
      "  4    2730          0.00          0.00       91.41    0.91\n",
      "  4    2740          0.00          0.00       91.68    0.92\n",
      "  4    2750          0.00          0.00       91.66    0.92\n",
      "  4    2760          0.00          0.00       91.58    0.92\n",
      "  4    2770          0.00          0.00       91.57    0.92\n",
      "  4    2780          0.00          0.00       91.58    0.92\n",
      "  4    2790          0.00          0.00       91.67    0.92\n",
      "  4    2800          0.00          0.00       91.82    0.92\n",
      "  4    2810          0.00          0.00       91.76    0.92\n",
      "  4    2820          0.00          0.00       91.59    0.92\n",
      "  4    2830          0.00          0.00       91.50    0.91\n",
      "  4    2840          0.00          0.00       91.60    0.92\n",
      "  4    2850          0.00          0.00       91.62    0.92\n",
      "  4    2860          0.00          0.00       91.51    0.92\n",
      "  4    2870          0.00          0.00       91.57    0.92\n",
      "  4    2880          0.00          0.00       91.40    0.91\n",
      "  4    2890          0.00          0.00       91.21    0.91\n",
      "  4    2900          0.00          0.00       91.23    0.91\n",
      "  4    2910          0.00          0.00       91.43    0.91\n",
      "  4    2920          0.00          0.00       91.58    0.92\n",
      "  4    2930          0.00          0.00       91.58    0.92\n",
      "  4    2940          0.00          0.00       91.70    0.92\n",
      "  4    2950          0.00          0.00       91.76    0.92\n",
      "  5    2960          0.00          0.00       91.93    0.92\n",
      "  5    2970          0.00          0.00       91.94    0.92\n",
      "  5    2980          0.00          0.00       92.02    0.92\n",
      "  5    2990          0.00          0.00       91.94    0.92\n",
      "  5    3000          0.00          0.00       91.92    0.92\n",
      "  5    3010          0.00          0.00       91.59    0.92\n",
      "  5    3020          0.00          0.00       91.75    0.92\n",
      "  5    3030          0.00          0.00       91.79    0.92\n",
      "  5    3040          0.00          0.00       91.81    0.92\n",
      "  5    3050          0.00          0.00       91.78    0.92\n",
      "  5    3060          0.00          0.00       91.66    0.92\n",
      "  5    3070          0.00          0.00       91.46    0.91\n",
      "  5    3080          0.00          0.00       91.41    0.91\n",
      "  5    3090          0.00          0.00       91.45    0.91\n",
      "  5    3100          0.00          0.00       91.52    0.92\n",
      "  5    3110          0.00          0.00       91.68    0.92\n",
      "  5    3120          0.00          0.00       91.62    0.92\n",
      "  5    3130          0.00          0.00       91.59    0.92\n",
      "  5    3140          0.00          0.00       91.51    0.92\n",
      "  5    3150          0.00          0.00       91.64    0.92\n",
      "  5    3160          0.00          0.00       91.65    0.92\n",
      "  5    3170          0.00          0.00       91.46    0.91\n",
      "  5    3180          0.00          0.00       91.58    0.92\n",
      "  5    3190          0.00          0.00       91.36    0.91\n",
      "  5    3200          0.00          0.00       91.19    0.91\n",
      "  5    3210          0.00          0.00       91.34    0.91\n",
      "  5    3220          0.00          0.00       91.30    0.91\n",
      "  5    3230          0.00          0.00       91.17    0.91\n",
      "  5    3240          0.00          0.00       91.30    0.91\n",
      "  5    3250          0.00          0.00       91.20    0.91\n",
      "  6    3260          0.00          0.00       91.19    0.91\n",
      "  6    3270          0.00          0.00       91.30    0.91\n",
      "  6    3280          0.00          0.00       91.30    0.91\n",
      "  6    3290          0.00          0.00       91.31    0.91\n",
      "  6    3300          0.00          0.00       91.37    0.91\n",
      "  6    3310          0.00          0.00       91.37    0.91\n",
      "  6    3320          0.00          0.00       91.49    0.91\n",
      "  6    3330          0.00          0.00       91.50    0.91\n",
      "  6    3340          0.00          0.00       91.41    0.91\n",
      "  6    3350          0.00          0.00       91.30    0.91\n",
      "  6    3360          0.00          0.00       91.15    0.91\n",
      "  6    3370          0.00          0.00       91.23    0.91\n",
      "  6    3380          0.00          0.00       91.23    0.91\n",
      "  6    3390          0.00          0.00       91.21    0.91\n",
      "  6    3400          0.00          0.00       91.08    0.91\n",
      "  6    3410          0.00          0.00       91.31    0.91\n",
      "  6    3420          0.00          0.00       91.53    0.92\n",
      "  6    3430          0.00          0.00       91.63    0.92\n",
      "  6    3440          0.00          0.00       91.64    0.92\n",
      "  6    3450          0.00          0.00       91.61    0.92\n",
      "  6    3460          0.00          0.00       91.61    0.92\n",
      "  6    3470          0.00          0.00       91.49    0.91\n",
      "  6    3480          0.00          0.00       91.44    0.91\n",
      "  6    3490          0.00          0.00       91.34    0.91\n",
      "  6    3500          0.00          0.00       91.36    0.91\n",
      "  6    3510          0.00          0.00       91.45    0.91\n",
      "  6    3520          0.00          0.00       91.50    0.91\n",
      "  6    3530          0.00          0.00       91.39    0.91\n",
      "  6    3540          0.00          0.00       91.29    0.91\n",
      "  7    3550          0.00          0.00       91.34    0.91\n",
      "  7    3560          0.00          0.00       91.37    0.91\n",
      "  7    3570          0.00          0.00       91.35    0.91\n",
      "  7    3580          0.00          0.00       91.16    0.91\n",
      "  7    3590          0.00          0.00       91.30    0.91\n",
      "  7    3600          0.00          0.00       91.41    0.91\n",
      "  7    3610          0.00          0.00       91.07    0.91\n",
      "  7    3620          0.00          0.00       91.08    0.91\n",
      "  7    3630          0.00          0.00       91.13    0.91\n",
      "  7    3640          0.00          0.00       91.20    0.91\n",
      "  7    3650          0.00          0.00       91.19    0.91\n",
      "  7    3660          0.00          0.00       91.24    0.91\n",
      "  7    3670          0.00          0.00       91.24    0.91\n",
      "  7    3680          0.00          0.00       91.19    0.91\n",
      "  7    3690          0.00          0.00       91.21    0.91\n",
      "  7    3700          0.00          0.00       91.27    0.91\n",
      "  7    3710          0.00          0.00       91.34    0.91\n",
      "  7    3720          0.00          0.00       91.34    0.91\n",
      "  7    3730          0.00          0.00       91.38    0.91\n",
      "  7    3740          0.00          0.00       91.58    0.92\n",
      "  7    3750          0.00          0.00       91.54    0.92\n",
      "  7    3760          0.00          0.00       91.50    0.92\n",
      "  7    3770          0.00          0.00       91.45    0.91\n",
      "  7    3780          0.00          0.00       91.39    0.91\n",
      "  7    3790          0.00          0.00       91.41    0.91\n",
      "  7    3800          0.00          0.00       91.30    0.91\n",
      "  7    3810          0.00          0.00       91.14    0.91\n",
      "  7    3820          0.00          0.00       91.03    0.91\n",
      "  7    3830          0.00          0.00       91.02    0.91\n",
      "  7    3840          0.00          0.00       91.19    0.91\n",
      "  8    3850          0.00          0.00       91.29    0.91\n",
      "  8    3860          0.00          0.00       91.41    0.91\n",
      "  8    3870          0.00          0.00       91.20    0.91\n",
      "  8    3880          0.00          0.00       91.29    0.91\n",
      "  8    3890          0.00          0.00       91.19    0.91\n",
      "  8    3900          0.00          0.00       90.99    0.91\n",
      "  8    3910          0.00          0.00       91.06    0.91\n",
      "  8    3920          0.00          0.00       91.09    0.91\n",
      "  8    3930          0.00          0.00       91.00    0.91\n",
      "  8    3940          0.00          0.00       91.04    0.91\n",
      "  8    3950          0.00          0.00       90.97    0.91\n",
      "  8    3960          0.00          0.00       90.92    0.91\n",
      "  8    3970          0.00          0.00       90.97    0.91\n",
      "  8    3980          0.00          0.00       91.13    0.91\n",
      "  8    3990          0.00          0.00       91.27    0.91\n",
      "  8    4000          0.00          0.00       91.23    0.91\n",
      "  8    4010          0.00          0.00       91.32    0.91\n",
      "  8    4020          0.00          0.00       91.25    0.91\n",
      "  8    4030          0.00          0.00       91.16    0.91\n",
      "  8    4040          0.00          0.00       91.28    0.91\n",
      "  8    4050          0.00          0.00       91.38    0.91\n",
      "  8    4060          0.00          0.00       91.35    0.91\n",
      "  8    4070          0.00          0.00       91.42    0.91\n",
      "  8    4080          0.00          0.00       91.46    0.91\n",
      "  8    4090          0.00          0.00       91.58    0.92\n",
      "  8    4100          0.00          0.00       91.59    0.92\n",
      "  8    4110          0.00          0.00       91.52    0.92\n",
      "  8    4120          0.00          0.00       91.53    0.92\n",
      "  8    4130          0.00          0.00       91.44    0.91\n",
      "  8    4140          0.00          0.00       91.53    0.92\n",
      "  9    4150          0.00          0.00       91.42    0.91\n",
      "  9    4160          0.00          0.00       91.18    0.91\n",
      "  9    4170          0.00          0.00       91.04    0.91\n",
      "  9    4180          0.00          0.00       91.09    0.91\n",
      "  9    4190          0.00          0.00       91.05    0.91\n",
      "  9    4200          0.00          0.00       91.01    0.91\n",
      "  9    4210          0.00          0.00       90.96    0.91\n",
      "  9    4220          0.00          0.00       91.07    0.91\n",
      "  9    4230          0.00          0.00       91.07    0.91\n",
      "  9    4240          0.00          0.00       91.00    0.91\n",
      "  9    4250          0.00          0.00       91.07    0.91\n",
      "  9    4260          0.00          0.00       91.20    0.91\n",
      "  9    4270          0.00          0.00       91.10    0.91\n",
      "  9    4280          0.00          0.00       91.21    0.91\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 2569\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/garza/notebooks/aicamp/yelp-project/wandb/run-20210322_224254-31v8ob3l/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/garza/notebooks/aicamp/yelp-project/wandb/run-20210322_224254-31v8ob3l/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               score 0.91215\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _runtime 15012\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          _timestamp 1616485986\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               _step 1286\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        loss_tok2vec 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        loss_textcat 0.00014\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           token_acc 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             token_p 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             token_r 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             token_f 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          cats_score 0.91215\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     cats_score_desc macro AUC\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        cats_micro_p 0.8344\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        cats_micro_r 0.8344\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        cats_micro_f 0.8344\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        cats_macro_p 0.83588\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        cats_macro_r 0.8344\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        cats_macro_f 0.83422\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      cats_macro_auc 0.91215\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               speed 15950.7745\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            score ‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         _runtime ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       _timestamp ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     loss_tok2vec ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     loss_textcat ‚ñá‚ñà‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        token_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          token_p ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          token_r ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          token_f ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       cats_score ‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     cats_micro_p ‚ñÅ‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     cats_micro_r ‚ñÅ‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     cats_micro_f ‚ñÅ‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     cats_macro_p ‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     cats_macro_r ‚ñÅ‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     cats_macro_f ‚ñÅ‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   cats_macro_auc ‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            speed ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÜ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mtrue-bird-19\u001b[0m: \u001b[34mhttps://wandb.ai/garza/yelp-polarity/runs/31v8ob3l\u001b[0m\n",
      "\u001b[38;5;2m‚úî Saved pipeline to output directory\u001b[0m\n",
      "models/textCatEnsemble/model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train ./config/config-TextCatEnsemble.cfg --output ./models/textCatEnsemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-blond",
   "metadata": {},
   "source": [
    "# Evaluate our best model output and save metrics to disk\n",
    "\n",
    "For hyperparameter tuning, experimented with different BOW attributes and TexCatEnsemble parameters:\n",
    "\n",
    "- ngram_size = 4 (TODO: should try again with ngram_size = 2 after doign BOW analysis)\n",
    "- adjusted width to 128, nominal performance gain\n",
    "- tok2vec model embed attributed modified to include:\n",
    "    - \"ORTH\", \"LOWER\", \"NORM\", \"PREFIX\", \"SUFFIX\", \"SHAPE\", \"ID\"\n",
    "    - [2000, 2000, 1000, 1000, 1000, 1000]\n",
    "\n",
    "Also adjusted the width size from 64 to 96, which only resulted in a nominal increase in performance.\n",
    "\n",
    "Training was tested on training datasets of 100, 500, 1000 and finally, 5000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "requested-subdivision",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4m‚Ñπ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Results ==================================\u001b[0m\n",
      "\n",
      "TOK                   100.00\n",
      "TEXTCAT (macro AUC)   92.85 \n",
      "SPEED                 15153 \n",
      "\n",
      "\u001b[1m\n",
      "=========================== Textcat F (per label) ===========================\u001b[0m\n",
      "\n",
      "               P       R       F\n",
      "positive   87.52   81.36   84.33\n",
      "negative   82.59   88.40   85.39\n",
      "\n",
      "\u001b[1m\n",
      "======================== Textcat ROC AUC (per label) ========================\u001b[0m\n",
      "\n",
      "           ROC AUC\n",
      "positive      0.93\n",
      "negative      0.93\n",
      "\n",
      "\u001b[38;5;2m‚úî Saved results to evaluate/model-textCatEnsemble-metrics.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy evaluate ./models/textCatEnsemble/model-best ./data/dev.spacy --output ./evaluate/model-textCatEnsemble-metrics.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "noble-adobe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "============================ Data file validation ============================\u001b[0m\n",
      "\u001b[38;5;2m‚úî Corpus is loadable\u001b[0m\n",
      "\u001b[38;5;2m‚úî Pipeline can be initialized with data\u001b[0m\n",
      "\u001b[1m\n",
      "=============================== Training stats ===============================\u001b[0m\n",
      "Language: en\n",
      "Training pipeline: tok2vec, textcat\n",
      "5000 training docs\n",
      "5000 evaluation docs\n",
      "\u001b[38;5;2m‚úî No overlap between training and evaluation data\u001b[0m\n",
      "\u001b[1m\n",
      "============================== Vocab & Vectors ==============================\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ 282161 total word(s) in the data (18163 unique)\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ No word vectors present in the package\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Summary ==================================\u001b[0m\n",
      "\u001b[38;5;2m‚úî 3 checks passed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy debug data ./config/config-TextCatEnsemble.cfg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-heavy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
